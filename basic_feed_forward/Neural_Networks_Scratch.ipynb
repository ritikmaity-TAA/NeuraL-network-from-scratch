{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-33hxDpoHBRF",
        "outputId": "abc3c03f-f993-4d28-9b0f-e3cc23fa1223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "numpy: 2.0.2\n",
            "matplotlib: 3.10.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "print(\"python:\", sys.version)\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"matplotlib:\", matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation involved in a single neuron"
      ],
      "metadata": {
        "id": "MWMaHf9w1S5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 2, 3] #[1.2, 5.2, 2.1]\n",
        "weights = [0.2, 0.8 , -0.5]\n",
        "\n",
        "bias = 2\n",
        "\n",
        "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFYh5FByy94I",
        "outputId": "9dc7b7f1-c097-4cea-9cf1-2832f001011a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation of 3 neuron with 4 inputs"
      ],
      "metadata": {
        "id": "JOhGr3Iw1-tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "bias1 = 2\n",
        "bias2 = 3\n",
        "bias3 = 0.5\n",
        "\n",
        "output = [\n",
        "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1, #output of neuron1\n",
        "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2, #output of neuron2\n",
        "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3, #output of neuron3\n",
        "]\n",
        "\n",
        "print(output)\n",
        "\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights=[\n",
        "    [0.2, 0.8, -0.5, 1.0],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_output=[]\n",
        "\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "  #print(\"Neuron Weights:\",neuron_weights, \"\\nNeuron Bias:\",neuron_bias)\n",
        "      # Neuron Weights: [0.2, 0.8, -0.5, 1.0]\n",
        "      # Neuron Bias: 2\n",
        "      # Neuron Weights: [0.5, -0.91, 0.26, -0.5]\n",
        "      # Neuron Bias: 3\n",
        "      # Neuron Weights: [-0.26, -0.27, 0.17, 0.87]\n",
        "      # Neuron Bias: 0.5\n",
        "  neuron_output = 0\n",
        "  for n_input, weight in zip(inputs, neuron_weights):\n",
        "    \"\"\"print(\"n_input:\",n_input,\"weight:\",weight)\n",
        "    n_input: 1 weight: 0.2\n",
        "    n_input: 2 weight: 0.8\n",
        "    n_input: 3 weight: -0.5\n",
        "    n_input: 2.5 weight: 1.0\n",
        "    n_input: 1 weight: 0.5\n",
        "    n_input: 2 weight: -0.91\n",
        "    n_input: 3 weight: 0.26\n",
        "    n_input: 2.5 weight: -0.5\n",
        "    n_input: 1 weight: -0.26\n",
        "    n_input: 2 weight: -0.27\n",
        "    n_input: 3 weight: 0.17\n",
        "    n_input: 2.5 weight: 0.87\"\"\"\n",
        "    neuron_output += n_input*weight\n",
        "  neuron_output +=neuron_bias\n",
        "  layer_output.append(neuron_output)\n",
        "\n",
        "print(layer_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvpxnLcK0jem",
        "outputId": "750ab605-86d6-4c82-8ba1-01ab8781707c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n",
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using numpy, single neuron"
      ],
      "metadata": {
        "id": "YqyRCp3dMd0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2\n",
        "\n",
        "output = np.dot( weights , inputs) + bias\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFG1hEJm4QmL",
        "outputId": "54da2fe9-2012-49b8-95c2-e949bb2744ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Numpy, A neuron layer"
      ],
      "metadata": {
        "id": "briRvtZSNJkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "print(np.shape(inputs))\n",
        "\n",
        "weights=[\n",
        "    [0.2, 0.8, -0.5, 1.0],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "\n",
        "print(np.shape(weights))\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "output = np.dot(weights,inputs) + biases\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz4OQhrtMZul",
        "outputId": "480ed542-fc1f-4687-aa76-6e8fffda27ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n",
            "(3, 4)\n",
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multiple inputs and mutiple neurons (shape)"
      ],
      "metadata": {
        "id": "nSkw_oYrg1vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[1, 2, 3, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]\n",
        "]\n",
        "print(np.shape(inputs))\n",
        "\n",
        "weights=[\n",
        "    [0.2, 0.8, -0.5, 1.0],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "\n",
        "print(np.shape(weights))\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "output = np.dot(inputs,np.array(weights).T) + biases #Transpose is performed to apply matrix multiplication\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo9xP3ssMpF6",
        "outputId": "f750b6a3-1145-4531-8ee4-362995da0890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "(3, 4)\n",
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Neural Network with actual code"
      ],
      "metadata": {
        "id": "GORFpbuv-4Pg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79cbe66"
      },
      "source": [
        "### Applying ReLU Activation Function\n",
        "\n",
        "The Rectified Linear Unit (ReLU) is a widely used activation function in neural networks, primarily for hidden layers. It introduces non-linearity into the model, allowing it to learn more complex patterns than linear activation functions.\n",
        "\n",
        "The formula for the ReLU function is simple:\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
        "\n",
        "Where:\n",
        "- $x$ is the input value (output from the dense layer).\n",
        "\n",
        "This means that if the input $x$ is positive, the output is $x$ itself. If the input is zero or negative, the output is zero. This simplicity makes ReLU computationally efficient and helps mitigate the vanishing gradient problem, which can occur with other activation functions like sigmoid or tanh.\n",
        "\n",
        "The `Activation_ReLU` class implemented below applies this function to the output of a dense layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "X = [[1, 2, 3, 2.5],\n",
        "     [2.0, 5.0, -1.0, 2.0],\n",
        "     [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) #reason for 0.10* is documented below\n",
        "    self.biases = np.zeros((1,n_neurons)) #1st param: is shape itself thus (1,n_neurons) is given as the shape, since no. of bias = no. of neurons\n",
        "    \"\"\"\n",
        "        print(np.random.randn(4,3))\n",
        "        [[ 1.76405235  0.40015721  0.97873798]\n",
        "        [ 2.2408932   1.86755799 -0.97727788]\n",
        "        [ 0.95008842 -0.15135721 -0.10321885]\n",
        "        [ 0.4105985   0.14404357  1.45427351]]\n",
        "\n",
        "        Since the values are exceeding 1, we are taking it down around the range of -1 to 1\n",
        "    \"\"\"\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0,inputs) #(if x<=0 then 0, else x)\n",
        "\n",
        "Layer1 = Layer_Dense(4,5)\n",
        "Layer2 = Layer_Dense(5,2) #no of input should be equal to number of output from previous layer\n",
        "\n",
        "Layer1.forward(X)\n",
        "Layer2.forward(Layer1.output)\n",
        "print(\"Output from Layer2:\\n\",Layer2.output)\n",
        "\n",
        "Output = Activation_ReLU()\n",
        "Output.forward(Layer2.output)\n",
        "print(\"Output from Layer2 after activation function:\\n\",Output.output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c7774a-4f3d-473e-83e6-ac70464582cf",
        "id": "gZc2WDMvpJj0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from Layer2:\n",
            " [[ 0.148296   -0.08397602]\n",
            " [ 0.14100315 -0.01340469]\n",
            " [ 0.20124979 -0.07290616]]\n",
            "Output from Layer2 after activation function:\n",
            " [[0.148296   0.        ]\n",
            " [0.14100315 0.        ]\n",
            " [0.20124979 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using nnfs package for dataset and same seed value along with computation in numpy (dot product)"
      ],
      "metadata": {
        "id": "TDkRE2n9_zHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "id": "n04TGbq8pDC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f27526-487b-4d38-c2df-129ad1aa8ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()\n",
        "\n",
        "X ,y = spiral_data(100,3)  #100 feature-set of 3 classes, X is the data and y is the label\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = np.random.randn(n_inputs, n_neurons)\n",
        "    self.bias = np.zeros((1,n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.bias\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "layer1 = Layer_Dense(2,5) #number of features = 2 since, there are x & y coords for the dataset\n",
        "layer2 = Layer_Dense(5,2)\n",
        "\n",
        "layer1.forward(X)\n",
        "print(\"Output of layer1:\\n\",layer1.output[:5])\n",
        "\n",
        "layer2.forward(layer1.output)\n",
        "print(\"Output of layer2:\\n\",layer2.output[:5])\n",
        "\n",
        "layer2_output = Activation_ReLU()\n",
        "layer2_output.forward(layer2.output)\n",
        "print(\"Output of layer2 after activation function:\\n\",layer2_output.output[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRkTkYkIACs2",
        "outputId": "87865f33-7fe1-4f73-98dc-64d67000c2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of layer1:\n",
            " [[ 0.          0.          0.          0.          0.        ]\n",
            " [-0.00835816 -0.00790404 -0.01334522  0.00465504  0.00045685]\n",
            " [-0.02399945  0.00059347 -0.02248083  0.00203573  0.00610024]\n",
            " [-0.0412122   0.04376721 -0.00953227 -0.01730223  0.0192649 ]\n",
            " [-0.05566051  0.05273885 -0.01720788 -0.02026777  0.02470861]]\n",
            "Output of layer2:\n",
            " [[ 0.          0.        ]\n",
            " [ 0.01493582 -0.00587628]\n",
            " [ 0.01073066 -0.0296586 ]\n",
            " [-0.04059668 -0.07439873]\n",
            " [-0.04603062 -0.09698327]]\n",
            "Output of layer2 after activation function:\n",
            " [[0.         0.        ]\n",
            " [0.01493582 0.        ]\n",
            " [0.01073066 0.        ]\n",
            " [0.         0.        ]\n",
            " [0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9608b7ad"
      },
      "source": [
        "### Applying Softmax Activation Function\n",
        "\n",
        "Softmax is an activation function often used in the output layer of a neural network for multi-class classification problems. It takes a vector of arbitrary real numbers and transforms them into a probability distribution, where the sum of the probabilities for each class is 1.\n",
        "\n",
        "The formula for the Softmax function for a single output $j$ in a layer of $K$ outputs is:\n",
        "\n",
        "$$\\text{Softmax}(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$$\n",
        "\n",
        "Where:\n",
        "- $z_j$ is the input value (output from the previous dense layer) for class $j$.\n",
        "- $e$ is Euler's number (the base of the natural logarithm).\n",
        "- $\\sum_{k=1}^{K} e^{z_k}$ is the sum of the exponentials of all input values in the layer.\n",
        "\n",
        "In practice, to prevent numerical instability from very large input values (leading to very large exponentials), a common optimization is to subtract the maximum value from all inputs before exponentiation:\n",
        "\n",
        "$$\\text{Softmax}(z_j) = \\frac{e^{z_j - \\max(z)}}{\\sum_{k=1}^{K} e^{z_k - \\max(z)}}$$\n",
        "\n",
        "This normalization step ensures that the exponential values remain within a manageable range, preventing potential overflow issues without changing the resulting probability distribution. The `Activation_Softmax` class implemented below includes this optimization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnfs.init()\n",
        "\n",
        "layer_outputs = [[4.8,1.21,2.385],\n",
        "                 [8.9,-1.81,0.3],\n",
        "                 [1.41,1.051,0.026]]\n",
        "\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print(\"exp values:\\n\",exp_values)\n",
        "norm_values = exp_values / np.sum(exp_values,axis=1,keepdims=True) #performing normalization\n",
        "#axis=None : sum of all individual values, axis=0: sum of all columns, axis=1: sum of all rows | keepdims : keep the dimension of input in the output\n",
        "\n",
        "print(\"\\nnorm values:\\n\",norm_values)\n",
        "print(\"\\nSum of all values:\\n\",np.sum(norm_values,axis=1,keepdims=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsApcaN6DPXJ",
        "outputId": "90f000ba-be58-446a-eb75-9197b3e2ed10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exp values:\n",
            " [[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
            " [7.33197354e+03 1.63654137e-01 1.34985881e+00]\n",
            " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
            "\n",
            "norm values:\n",
            " [[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99793616e-01 2.23160054e-05 1.84067797e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n",
            "\n",
            "Sum of all values:\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnfs.init()\n",
        "from nnfs.datasets import spiral_data\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values  = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    self.output = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
        "\n",
        "X, y = spiral_data(samples=100,classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3,3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(\"top 5 output:\\n\",activation2.output[:5])\n",
        "print(\"\\nmax of the output of 5 row-wise:\\n\",np.max(activation2.output,axis=1,keepdims=True)[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYJBxtBoVdKY",
        "outputId": "0dab525c-36ea-43cc-9928-a56239b797fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 5 output:\n",
            " [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33173057 0.3318284  0.336441  ]\n",
            " [0.3288542  0.3291243  0.3420215 ]\n",
            " [0.32587105 0.32631534 0.34781358]\n",
            " [0.32318372 0.32378128 0.35303497]]\n",
            "\n",
            "max of the output of 5 row-wise:\n",
            " [[0.33333334]\n",
            " [0.336441  ]\n",
            " [0.3420215 ]\n",
            " [0.34781358]\n",
            " [0.35303497]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6012b3fd"
      },
      "source": [
        "### Implementing Loss (Categorical Cross-Entropy)\n",
        "\n",
        "Categorical Cross-Entropy is a common loss function used in classification tasks, particularly when there are multiple classes. It quantifies the difference between predicted probabilities (from the Softmax activation) and the true class labels.\n",
        "\n",
        "The formula for Categorical Cross-Entropy for a single sample is:\n",
        "\n",
        "$$L = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "Where:\n",
        "- $L$ is the loss.\n",
        "- $C$ is the number of classes.\n",
        "- $y_i$ is the true probability for class $i$ (usually 1 for the correct class and 0 for others in one-hot encoding).\n",
        "- $\\hat{y}_i$ is the predicted probability for class $i$ (output from the Softmax activation).\n",
        "\n",
        "In the following code, we manually calculate this loss for a single example to illustrate the concept. For actual neural network training, we use optimized class implementations that handle batches of data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "softmax_output = [0.7,0.1,0.2]\n",
        "target_output = [1,0,0] #this is one hot encoding given target output index is [0] thus OHE -> [1,0,0]\n",
        "\n",
        "loss_output = - (\n",
        "    math.log(softmax_output[0])*target_output[0]\n",
        "    + math.log(softmax_output[1])*target_output[1]\n",
        "    + math.log(softmax_output[2])*target_output[2]\n",
        "              )\n",
        "\n",
        "print(loss_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhO6iNbbbreK",
        "outputId": "55334c4d-f67c-473f-9dac-8a7d8f94907b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "nnfs.init()\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "class Layer_Dense:\n",
        "  \"\"\"Implements a dense neural network layer.\"\"\"\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  \"\"\"Implements the Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  \"\"\"Implements the Softmax activation function, typically used for output layers in classification.\"\"\"\n",
        "  def forward(self,inputs):\n",
        "    exp_values  = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    self.output = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
        "\n",
        "class Loss:\n",
        "  \"\"\"Base class for loss functions.\"\"\"\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoreicalCrossEntropy(Loss):\n",
        "  \"\"\"Implements the Categorical Cross-Entropy loss function.\"\"\"\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7, 1-1e-7) #1e-7, 1-1e-7 so that log(0) is not encountered\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "      \"\"\"\n",
        "      [[0.7, 0.1, 0.2],    range(samples)->     [0,  y_true->  [0,         [0.7,  we are taking the range of y_true and thus mapping\n",
        "      [0.1, 0.5, 0.4],                           1,   ,         1,     =    0.5,  each row's target value with the predicted value\n",
        "      [0.02, 0.9, 0.08]]                         2]             1]          0.9]\n",
        "      \"\"\"\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "      \"\"\"\n",
        "      np.sum([[0.7, 0.1, 0.2], <- y_pred    y_true(OHE)-> [[1, 0, 0],               [0.7,      we are multiplying y_true(OHE) & y_pred_clipped thus\n",
        "      [0.1, 0.5, 0.4],                    *               [0, 1, 0]           =      0.5,      only those features remain in y_pred_clipped which are 1(target_class)\n",
        "      [0.02, 0.9, 0.08]]                                  [0, 1, 0]], axis=1)        0.9]      thus sum for each row become the y_pred value for that row\n",
        "      \"\"\"\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "X, y = spiral_data(samples=100,classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,5)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(5,3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])\n",
        "loss_func = Loss_CategoreicalCrossEntropy()\n",
        "loss = loss_func.calculate(activation2.output,y)\n",
        "print(\"\\nLoss:\",loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKE0nE19pdf-",
        "outputId": "a58b15e6-e1a7-4558-832c-3cc68da1a09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33334148 0.3333302  0.33332834]\n",
            " [0.33335316 0.33332598 0.33332086]\n",
            " [0.333332   0.33330762 0.3333604 ]\n",
            " [0.33333603 0.33330083 0.33336315]]\n",
            "\n",
            "Loss: 1.0988972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "nnfs.init()\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "class Layer_Dense:\n",
        "  \"\"\"Implements a dense neural network layer.\"\"\"\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.1 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  \"\"\"Implements the Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  \"\"\"Implements the Softmax activation function, typically used for output layers in classification.\"\"\"\n",
        "  def forward(self,inputs):\n",
        "    exp_values  = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    self.output = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
        "\n",
        "class Loss:\n",
        "  \"\"\"Base class for loss functions.\"\"\"\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoreicalCrossEntropy(Loss):\n",
        "  \"\"\"Implements the Categorical Cross-Entropy loss function.\"\"\"\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7, 1-1e-7) #1e-7, 1-1e-7 so that log(0) is not encountered\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "X, y = spiral_data(samples=100,classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,6)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(6,4)\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(4,3)\n",
        "activation3 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "print(activation3.output[:5])\n",
        "loss_func = Loss_CategoreicalCrossEntropy()\n",
        "loss = loss_func.calculate(activation2.output,y)\n",
        "print(\"\\nLoss:\",loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7cd108-2000-4846-e5cb-b455ad742671",
        "id": "Ib2_bMMVnW0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333263 0.33333406 0.33333334]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3333354  0.3333322  0.33333233]\n",
            " [0.33333594 0.33333176 0.33333233]]\n",
            "\n",
            "Loss: 14.589445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results With Different Values and Layers\n",
        "https://docs.google.com/spreadsheets/d/1bBG8H9Hab5VEZQZ6b2kp7RpDjoK8KjznwjO0ItosJMI/edit?usp=sharing"
      ],
      "metadata": {
        "id": "MqhL5TVEos1x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9d3125e"
      },
      "source": [
        "### Approximating Derivatives and Tangent Lines\n",
        "\n",
        "This code demonstrates how to numerically approximate the derivative of a function and visualize its tangent line at a specific point. The function used for this example is $f(x) = 2x^2$.\n",
        "\n",
        "**Steps involved:**\n",
        "1.  **Define the function**: A Python function `f(x)` is defined to represent $2x^2$.\n",
        "2.  **Generate data for plotting**: `x` values from 0 to 50 are created with a small step, and corresponding `y` values are calculated using `f(x)` to plot the original curve.\n",
        "3.  **Choose a point for approximation**: An `x1` value (e.g., 2) is chosen to calculate the derivative and tangent line.\n",
        "4.  **Calculate a second point**: A small `p2_delta` (e.g., 0.0001) is added to `x1` to get `x2`, creating a tiny interval for approximation.\n",
        "5.  **Compute function values**: `y1 = f(x1)` and `y2 = f(x2)` are calculated.\n",
        "6.  **Approximate the derivative**: The derivative is approximated as the slope of the line connecting `(x1, y1)` and `(x2, y2)` using the formula: $ \\frac{\\Delta y}{\\Delta x} = \\frac{y2 - y1}{x2 - x1} $.\n",
        "7.  **Find the tangent line equation**: Using the point-slope form $y - y_1 = m(x - x_1)$, where $m$ is the approximate derivative, the intercept `b` for the tangent line ($y = mx + b$) is calculated.\n",
        "8.  **Define tangent line function**: A function `approximate_tangent_line(x)` is created to easily get y-values for the tangent line.\n",
        "9.  **Plotting**: The original function $f(x)$ and the approximated tangent line are plotted using `matplotlib` to visually confirm the approximation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x):\n",
        "  return 2*x**2\n",
        "\n",
        "x = np.arange(0, 50, 0.001)\n",
        "y = f(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "\n",
        "p2_delta = 0.0001\n",
        "x1 = 2\n",
        "x2 = x1 + p2_delta\n",
        "\n",
        "y1 = f(x1)\n",
        "y2 = f(x2)\n",
        "\n",
        "print((x1,y1), (x2,y2))\n",
        "\n",
        "approximate_derivate = (y2 - y1) / (x2 - x1)\n",
        "b = y2 - approximate_derivate * x2\n",
        "\n",
        "def approximate_tangent_line(x):\n",
        "  return approximate_derivate*x + b\n",
        "\n",
        "to_plot = [x1 - 0.9, x1, x1+0.9]\n",
        "plt.plot(to_plot, [approximate_tangent_line(point) for point in to_plot])\n",
        "\n",
        "\n",
        "print(\"Approximate derivative for f(x)\", f'where x = {x1} is {approximate_derivate}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "4axi606DqUgJ",
        "outputId": "9089ce1b-5bb9-48ad-d3f0-3a7c08be582f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 8) (2.0001, 8.000800020000002)\n",
            "Approximate derivative for f(x) where x = 2 is 8.000199999998785\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85YKJwtgjTAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}